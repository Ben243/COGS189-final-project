{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "e4294ca042d89edc07e75e48bdee0189d8edcdf6ebdc7bd53805b5ccb8107651"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 songs for each of 4 trials for each of 31 subjects = total of 1240 playings\n",
    "# 12 target classes: HAPPY, LOW VALENCE, LOW TENSION, LOW ENERGY, HIGH ENERGY, SURPRISE, FEAR, TENDER, ANGER, SAD, HIGH VALENCE, HIGH TENSION\n",
    "# ? Unknown how many playings fall under each target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne # package for reading edf data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, sosfiltfilt, sosfreqz  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 240.0                      # Hz; sampling rate\n",
    "dt = 1000/fs                    # ms; time between samples\n",
    "sdt = np.round(dt).astype(int); # rounded dt so that we can index samples\n",
    "hp = 0.1                        # Hz; our low cut for our bandpass\n",
    "lp = 30.                        # Hz; our high cut for our bandpass\n",
    "order = 3                       # filter order (functionally doubled)\n",
    "\n",
    "# Create our filter coefficient as as a second-order section\n",
    "# Note: by defining 'fs' we don't divide our windows by the Nyquist\n",
    "sos = butter(order, [hp, lp], analog = False, btype = 'band', output = 'sos', fs = fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing: data/sub-01/eeg/sub-01_task-run2_eeg.edf\n",
      "Extracting EDF parameters from c:\\Users\\Jack Sheridan\\Documents\\COGS189\\COGS189-final-project\\data\\sub-01\\eeg\\sub-01_task-run2_eeg.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-1ba19ebe4530>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# Going through all 10 playings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mplaying\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m             \u001b[0mrecording\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecordings\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mplaying\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;31m# Epoch and correct DC offset of signal pre-filtering\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Create empty lists to store our data. We'll conver them into np.arrays at the end\n",
    "train_data = []\n",
    "train_labels = []\n",
    "test_data = []\n",
    "\n",
    "test_set = np.random.choice(124, size=24, replace=False)\n",
    "\n",
    "# Going through all 124 trials\n",
    "for subject in range(1,32):\n",
    "    for trial in range(2,6):\n",
    "        fileName = f\"data/sub-{format(subject, '02x')}/eeg/sub-{format(subject, '02x')}_task-run{trial}_eeg.edf\"\n",
    "        print('Processing: ' + fileName)\n",
    "        data = mne.io.read_raw_edf(fileName)\n",
    "        trialRecording = data.to_data_frame().infer_objects()\n",
    "\n",
    "        labels = [] # Should be an array of 10 labels for each of the songs played in this trial\n",
    "        recordings = [] # Should be a 3D array of 10 recordings, each with 16000 rows and 20 columns\n",
    "        # Bens code to get individual playings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Going through all 10 playings\n",
    "        for playing in range(1,11):\n",
    "            recording = recordings[playing]\n",
    "\n",
    "            # Epoch and correct DC offset of signal pre-filtering\n",
    "            recording = recording_[e_s+t0:t0+e_e, :] - np.mean(recording_[e_s+t0:t0+e_e, :], 0)\n",
    "            recording = sosfiltfilt(sos, recording)\n",
    "            \n",
    "            # Now let's baseline correct\n",
    "            # data = data - np.mean(data[bl_s+np.abs(e_s):np.abs(e_s)+bl_e, :], 0)\n",
    "            \n",
    "            # Append data to correct locations\n",
    "            if s in test_set:\n",
    "                test_data.append(data)\n",
    "            else:\n",
    "                train_data.append(data)\n",
    "                train_labels.append(labels[playing]) # target or non-target\n",
    "                    \n",
    "print('Processing completed!')\n",
    "\n",
    "# Convert all of our lists into numpy arrays\n",
    "train_data = np.array(train_data)\n",
    "train_labels = np.array(train_labels)\n",
    "test_data = np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate target and non-target for plotting\n",
    "tar     = train_data[np.where(train_labels == 1)[0], :, :]\n",
    "non_tar = train_data[np.where(train_labels == 0)[0], :, :]\n",
    "\n",
    "print('We have %d target trials' % tar.shape[0])\n",
    "print('We have %d non-target trials' % non_tar.shape[0])\n",
    "\n",
    "# We'll take the average of all trials to create an averaged ERP\n",
    "tar_avg     = np.mean(tar, 0)\n",
    "non_tar_avg = np.mean(non_tar, 0)\n",
    "\n",
    "# Define channel of interest and create an array of time points\n",
    "chan = 'Cz' # let's plot Cz\n",
    "ch = np.where(channels == chan)[0][0]\n",
    "times = np.linspace(epoch_start, epoch_end, train_data.shape[1])\n",
    "\n",
    "# Initialize plot and calculate min and max y value\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 6))\n",
    "min_y = min(np.min(tar_avg), np.min(non_tar_avg))\n",
    "max_y = max(np.max(tar_avg), np.max(non_tar_avg))\n",
    "\n",
    "# Plot x and y axes\n",
    "plt.plot([np.min(times), np.max(times)], [0, 0], color='k');  # x-axis\n",
    "plt.plot([0, 0], [min_y, max_y], color='k');                  # y-axis\n",
    "\n",
    "# Plot our averaged ERPs\n",
    "plt.plot(times, tar_avg[:, ch], 'b', linewidth=4)\n",
    "plt.plot(times, non_tar_avg[:, ch], 'r', linewidth=4)\n",
    "\n",
    "# Highlight the baseline window and window of interest of our ERP\n",
    "baseline = patches.Rectangle([baseline_start, min_y], baseline_end, np.abs(min_y)+max_y, \n",
    "                             color='c', alpha=0.2)\n",
    "erp_win = patches.Rectangle([erp_start, min_y], erp_end-erp_start, np.abs(min_y)+max_y, \n",
    "                             color='lime', alpha=0.3)\n",
    "\n",
    "# Add our baseline and window of interest highlights\n",
    "ax.add_patch(baseline)\n",
    "ax.add_patch(erp_win)\n",
    "\n",
    "# Manually create legends since patches will corrupt default handles\n",
    "legend_ = [patches.Patch(color='b', label = 'Target (oddball)'),\n",
    "           patches.Patch(color='r', label = 'Non-target (standard)')]\n",
    "\n",
    "# Finalize plot and set a high DPI for a crisp, hi-res figure\n",
    "plt.xlabel('Time (msec)');\n",
    "plt.ylabel('Amplitude (A/D Units)');\n",
    "plt.legend(handles=legend_, loc=\"upper right\");\n",
    "plt.title('Event Related Potentials at channel %s' % chan);\n",
    "fig.set_dpi(216);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute the windowed means within erp_start and erp_end\n",
    "num_points = 5; # we will divide our window into num_points means\n",
    "\n",
    "# Define a simple windowed means function\n",
    "def wm(x, start, end, num_points):\n",
    "    num_trials = x.shape[0] # assumes first dem is numb observations\n",
    "    w = np.round((start+end)/num_points).astype(int)\n",
    "    y = np.zeros((num_points, x.shape[-1], num_trials)) # assumes num chans as last dimension\n",
    "    for i in range(0, num_points):\n",
    "        s = start + (w * i)\n",
    "        e = end   + (w * i)\n",
    "        y[i, :, :] = np.mean(x[:, s:e, :], 1).T\n",
    "    return y\n",
    "\n",
    "# Combine into a single train variable. Also create labels\n",
    "X_train    = wm(train_data, erp_s, erp_e, num_points)\n",
    "markers_train = np.vstack((train_labels, train_markers)).T\n",
    "y = train_labels\n",
    "\n",
    "# Now let's compute windowed means of our test data\n",
    "X_test = wm(test_data, erp_s, erp_e, num_points)\n",
    "markers_test = test_markers\n",
    "\n",
    "# Let's print out the shapes of our data\n",
    "print('X_train shape is: ' + str(X_train.shape))\n",
    "print('y shape is......: ' + str(y.shape))\n",
    "print('X_test shape is.: ' + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since our X is 3D, we must flatten our data. We will then transpose it for sklearn\n",
    "X_train = X_train.reshape(-1, X_train.shape[-1]).T\n",
    "X_test = X_test.reshape(-1, X_test.shape[-1]).T\n",
    "\n",
    "# Let's print out the new shape\n",
    "print('X_train shape is now: ' + str(X_train.shape))\n",
    "print('X_test  shape is now: ' + str(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our classifier (this may take a while via JupyterHub)\n",
    "clf_lsqrs = LinearDiscriminantAnalysis(solver = 'lsqr',  shrinkage = 'auto').fit(X_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do 5-fold cross validation\n",
    "score_lsqrs = cross_val_score(clf_lsqrs.fit(X_train, y), X_train, y, cv = 5)\n",
    "\n",
    "# We will print out the mean score\n",
    "print(\"solver = lsqr  accuracy: %f\" % np.mean(score_lsqrs))"
   ]
  }
 ]
}